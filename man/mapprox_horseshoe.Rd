% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/modified_approximate_algorithm.R
\name{mapprox_horseshoe}
\alias{mapprox_horseshoe}
\title{Run modified approximate MCMC algorithm for horseshoe prior}
\usage{
mapprox_horseshoe(
  X,
  y,
  iteration = 5000,
  a = 1/5,
  b = 10,
  s = 0.8,
  tau = 1,
  sigma2 = 1,
  w = 1,
  t = 10,
  alpha0 = 0,
  alpha1 = -4.6 * 10^(-4)
)
}
\arguments{
\item{X}{Design matrix, \eqn{X \in \mathbb{R}^{N \times p}}.}

\item{y}{Response vector, \eqn{y \in \mathbb{R}^{N}}.}

\item{iteration}{Number of samples to be drawn from the posterior.}

\item{a}{Parameter of the rejection sampler, and it is recommended to leave
it at the default value, \eqn{a = 1/5}.}

\item{b}{Parameter of the rejection sampler, and it is recommended to leave
it at the default value, \eqn{b = 10}.}

\item{s}{\eqn{s^{2}} is the variance of the MH proposal distribution set by
the user. 0.8 is a good default. If set to 0, the algorithm proceeds by
fixing the global shrinkage parameter \eqn{\tau} to the initial setting
value.}

\item{tau}{Initial value of the global shrinkage parameter \eqn{\tau} when
starting the algorithm. If argument \eqn{s} is set to 0 and this argument
is set, analysis of a fixed \eqn{\tau} is performed. Default is 1.}

\item{sigma2}{Default for error variance \eqn{\sigma^{2}} is 1.}

\item{w}{Parameter of gamma prior for \eqn{\sigma^{2}}. Default is 1.}

\item{t}{Threshold \eqn{\delta} update cycle for adaptive probability
method. default is 10.}

\item{alpha0}{Parameter \eqn{a_{0}} of adaptive probability,
\eqn{p(t) = exp[a_{0} + a_{1}t]}. default is 0.}

\item{alpha1}{Parameter \eqn{a_{1}} of adaptive probability,
\eqn{p(t) = exp[a_{0} + a_{1}t]}. default is \eqn{-4.6 \times 10^{-4}}.}
}
\value{
\item{BetaSamples}{Samples from the posterior of the parameter
\eqn{\beta}.}
\item{LambdaSamples}{Lambda samples through rejection sampling.}
\item{TauSamples}{Tau samples through MH algorithm.}
\item{Sigma2Samples}{Samples from the posterior of the parameter
\eqn{sigma^{2}}.}
\item{ActiveSamples}{Samples recording the number of columns that satisfy
condition \eqn{\tau^{2}\lambda^{2}_{j} > \delta,\ j=1,2,...,p} in this
algorithm.}
}
\description{
This function implements an approximate MCMC algorithm that reduces
computational cost in the same way as Johndrow et al. (2020). See
\code{\link{approx_horseshoe}}. Unlike approx_horseshoe, instead of using a
fixed threshold, it uses an adaptive probability method to update the
threshold estimated by applying the shrinkage weight of Piironen and Vehtari
(2017).
}
\details{
In this algorithm, the process of updating a new threshold is added by
applying the properties of the shrinkage weight \eqn{k_{j},\ j=1,2,...,p}
proposed by Piironen and Vehtari (2017). In the prior of \eqn{\beta_{j} \sim
N(0, \sigma^{2}\tau^{2}\lambda_{j}^{2}) = N(0, \sigma^{2}\xi^{-1}
\eta_{j}^{-1})}, the variable \eqn{m_{eff}} is defined as follows.

\deqn{k_{j} = 1/\left(1+n\xi^{-1}s_{j}^{2}\eta_{j}^{-1} \right),
\quad j=1,2,...,p, \\ m_{eff} = \sum_{j=1}^{p}{\left(1-k_{j} \right)}.}

The assumptions and notations for the model are the same as those in
\code{\link{Mhorseshoe}}, and \eqn{s_{j},\ j=1,2,...,p} are the diagonal
components of \eqn{X^{T}X}. For the zero components of \eqn{\beta},
\eqn{k_{j}} is derived close to 1, and nonzero's \eqn{k_{j}} is derived
close to 0, so the variable \eqn{m_{eff}} is called the effective number of
nonzero coefficients. In this algorithm, the threshold \eqn{\delta} is
updated to set \eqn{s_{\delta} = ceiling(m_{eff})}. \eqn{s_{\delta}} is the
same as \eqn{length(S)} in details of \code{\link{approx_horseshoe}}.

Adaptive probability is defined to satisfy
Theorem 5(diminishing adaptation condition) of Roberts and Rosenthal (2007).
at \eqn{T}th iteration,

\deqn{p(T) = exp[a_{0} + a_{1}T],\quad a_{1} < 0,
\quad u \sim U(0, 1), \\ if\ u < p(T),\ update\ \delta\ so\ that\
s_{\delta} = ceiling(m_{eff}).}

The default is \eqn{a_{0} = 0}, \eqn{a_{1} = -4.6 \times 10^{-4}}, and
under this condition, \eqn{p(10000) < 0.01} is satisfied.
}
\examples{
\dontrun{
# Making simulation data.
set.seed(123)
N <- 100
p <- 200
true_beta <- c(rep(1, 10), rep(0, 190))

X <- matrix(1, nrow = N, ncol = p) # Design matrix X.
for (i in 1:p) {
  X[, i] <- stats::rnorm(N, mean = 0, sd = 1)
}

y <- vector(mode = "numeric", length = N) # Response variable y.
e <- rnorm(N, mean = 0, sd = 2) # error term e.
for (i in 1:50) {
  y <- y + true_beta[i] * X[, i]
}
y <- y + e

# Run as default
result <- mapprox_horseshoe(X, y, iteration = 1000)

# posterior mean
post_mean <- apply(result$BetaSamples, MARGIN = 2, mean)

# Lower bound of the 95\% credible interval
post_leftCI <- apply(result$BetaSamples, MARGIN = 2,
                     quantile, probs = 0.025)

# Upper bound of the 95\% credible interval
post_rightCI <- apply(result$BetaSamples, MARGIN = 2,
                      quantile, probs = 0.975)}

}
\references{
Johndrow, J., Orenstein, P., & Bhattacharya, A. (2020).
Scalable Approximate MCMC Algorithms for the Horseshoe Prior. In Journal
of Machine Learning Research (Vol. 21).

Piironen, J., & Vehtari, A. (2017). Sparsity information and regularization
in the horseshoe and other shrinkage priors. Electronic Journal of
Statistics, 11, 5018-5051.

Roberts G, Rosenthal J. Coupling and ergodicity of adaptive Markov chain
Monte Carlo algorithms. J Appl Prob. 2007;44:458â€“475.
}
