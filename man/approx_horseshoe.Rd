% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/approximate_algorithm.R
\name{approx_horseshoe}
\alias{approx_horseshoe}
\title{Run approximate MCMC algorithm for horseshoe prior}
\usage{
approx_horseshoe(
  X,
  y,
  burn = 1000,
  iter = 5000,
  auto.threshold = TRUE,
  threshold = 0,
  tau = 1,
  s = 0.8,
  sigma2 = 1,
  w = 1,
  a = 0.2,
  b = 10,
  t = 10,
  adapt_p0 = 0,
  adapt_p1 = -4.6 * 10^(-4),
  alpha = 0.05
)
}
\arguments{
\item{X}{Design matrix, \eqn{X \in \mathbb{R}^{N \times p}}.}

\item{y}{Response vector, \eqn{y \in \mathbb{R}^{N}}.}

\item{iter}{Number of samples to be drawn from the posterior.}

\item{auto.threshold}{Argument for setting whether to use an algorithm that
automatically updates the threshold using adaptive probability.}

\item{threshold}{Threshold to be used in the approximate MCMC algorithm.
If you select 0(this is the default), the default is set according to the
sizes of N and p. if \eqn{p < N}, \eqn{\delta = 1/\sqrt{Np}}, else
\eqn{\delta = 1/p}. Or, you can set the value directly through this
argument. For more information about \eqn{\delta}, see
\code{\link{Mhorseshoe}} and 4.1 of Johndrow et al. (2020).}

\item{tau}{Initial value of the global shrinkage parameter \eqn{\tau} when
starting the algorithm. If argument \eqn{s} is set to 0 and this argument
is set, analysis of a fixed \eqn{\tau} is performed. Default is 1.}

\item{s}{\eqn{s^{2}} is the variance of the MH proposal distribution set by
the user. 0.8 is a good default. If set to 0, the algorithm proceeds by
fixing the global shrinkage parameter \eqn{\tau} to the initial setting
value.}

\item{sigma2}{Default for error variance \eqn{\sigma^{2}} is 1.}

\item{w}{Parameter of gamma prior for \eqn{\sigma^{2}}. Default is 1.}

\item{a}{Parameter of the rejection sampler, and it is recommended to leave
it at the default value, \eqn{a = 1/5}.}

\item{b}{Parameter of the rejection sampler, and it is recommended to leave
it at the default value, \eqn{b = 10}.}

\item{t}{Threshold update cycle for adaptive probability method when
auto.threshold is set to TRUE. default is 10.}

\item{adapt_p0}{Parameter \eqn{p_{0}} of adaptive probability,
\eqn{p(t) = exp[p_{0} + p_{1}t]}. default is 0.}

\item{adapt_p1}{Parameter \eqn{a_{1}} of adaptive probability,
\eqn{p(t) = exp[p_{0} + p_{1}t]}. default is \eqn{-4.6 \times 10^{-4}}.}
}
\value{
\item{BetaHat}{Posterior mean of \eqn{\beta}.}
\item{LeftCI}{Lower bound of credible interval for \eqn{\beta}.}
\item{RightCI}{Upper bound of credible interval for \eqn{\beta}.}
\item{Sigma2Hat}{Posterior mean of \eqn{\sigma^{2}}.}
\item{TauHat}{Posterior mean of \eqn{\tau}.}
\item{LambdaHat}{Posterior mean of \eqn{\lambda_{j},\ j=1,2,...p.}.}
\item{ActiveMean}{Average number of elements in the active set per iteration
in this algorithm.}
\item{BetaSamples}{Samples from the posterior of \eqn{\beta}.}
\item{LambdaSamples}{Lambda samples through rejection sampling.}
\item{TauSamples}{Tau samples through MH algorithm.}
\item{Sigma2Samples}{Samples from the posterior of the parameter
\eqn{sigma^{2}}.}
\item{ActiveSet}{Matrix indicating active elements as 1 and non-active
elements as 0 per iteration of the MCMC algorithm.}
}
\description{
In this function, The algorithm introduced in Section 2.2 of Johndrow et al.
(2020) is implemented, and is a horseshoe estimator that generally considers
the case where \eqn{p>>N}. The assumptions and notations for the model are
the same as those in \code{\link{Mhorseshoe}}. This algorithm introduces a
threshold and uses only a portion of the total \eqn{p} columns for matrix
multiplication, lowering the computational cost compared to the existing
horseshoe estimator. According to Section 3.2 of Johndrow et al. (2020), the
approximate MCMC algorithm applying the methodology constructs an
approximate Markov chain \eqn{P_{\epsilon}} that can converge to an exact
Markov chain \eqn{P}, and acceptable results were confirmed through
empirical analysis of simulation and real data.
}
\details{
This algorithm has the following changes compared to the exact algorithm:

\deqn{D_{\delta} = diag\left(\eta_{j}^{-1}1\left(\xi^{-1}\eta_{j}^{-1}
> \delta,\ j=1,2,...,p. \right) \right),}
\deqn{M_{\xi} \approx M_{\xi, \delta} = I_{N} + \xi^{-1}XD_{\delta}X^{T}.}

The set of columns that satisfies the condition(\eqn{\xi^{-1}\eta_{j}^{-1}
> \delta}) is defined as the active set, and let's define \eqn{S} as the
index set of the following columns.

\deqn{S = \{j\ |\ \xi^{-1}\eta_{j}^{-1} > \delta,\ j=1,2,...,p. \}.}

If \eqn{\xi^{-1}\eta_{j}^{-1}} is very small, the posterior of \eqn{\beta}
will have a mean and variance close to 0. Therefore, let's set
\eqn{\xi^{-1}\eta_{j}^{-1}} smaller than \eqn{\delta} to 0 and the size of
inverse \eqn{M_{\xi, \delta}} matrix is reduced as follows.

\deqn{length(S)=s_{\delta} \le p, \\ X_{S} \in R^{N \times s_{\delta}},
\quad D_{S} \in R^{s_{\delta} \times s_{\delta}}, \\ M_{\xi, \delta}^{-1} =
\left(I_{N} + \xi^{-1}X_{S}D_{S}X_{S}^{T} \right)^{-1}.}

\eqn{M_{\xi, \delta}^{-1}} can be expressed using the Woodbury identity
as follows.

\deqn{M_{\xi, \delta}^{-1} = I_{N} - X_{S}\left(\xi D_{S}^{-1} +
X_{S}^{T}X_{S} \right)^{-1}X_{S}^{T}.}

\eqn{M_{\xi, \delta}^{-1}}, which reduces the computational cost, is
applied to all parts of this algorithm, \eqn{\beta} samples are extracted
from the posterior using fast sampling(Bhattacharya et al.,2016) as follows.

\deqn{u \sim N_{p}(0, \xi^{-1}D),\quad f \sim N_{N}(0, I_{N}), \\
v = Xu + f,\quad v^{\star} = M_{\xi, \delta}^{-1}(y/\sigma - v), \\
\beta = \sigma(u + \xi^{-1}D_{\delta}X^{T}v^{\star}).}
}
\examples{
# Making simulation data.
set.seed(123)
N <- 50
p <- 100
true_beta <- c(rep(1, 10), rep(0, 90))

X <- matrix(1, nrow = N, ncol = p) # Design matrix X.
for (i in 1:p) {
  X[, i] <- stats::rnorm(N, mean = 0, sd = 1)
}

y <- vector(mode = "numeric", length = N) # Response variable y.
e <- rnorm(N, mean = 0, sd = 2) # error term e.
for (i in 1:10) {
  y <- y + true_beta[i] * X[, i]
}
y <- y + e

# Run with auto.threshold option
result <- approx_horseshoe(X, y, iteration = 1000)

# Run with fixed threshold
# result <- approx_horseshoe(X, y, iteration = 1000, auto.threshold = FALSE)

# posterior mean
betahat <- result$BetaHat

# Lower bound of the 95\% credible interval
post_leftCI <- result$LeftCI

# Upper bound of the 95\% credible interval
post_RightCI <- result$RightCI

}
\references{
Bhattacharya, A., Chakraborty, A., & Mallick, B. K. (2016).
Fast sampling with Gaussian scale mixture priors in high-dimensional
regression. Biometrika, asw042.

Johndrow, J., Orenstein, P., & Bhattacharya, A. (2020).
Scalable Approximate MCMC Algorithms for the Horseshoe Prior. In Journal
of Machine Learning Research (Vol. 21).
}
