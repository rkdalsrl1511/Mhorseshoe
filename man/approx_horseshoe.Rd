% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/approximate_algorithm.R
\name{approx_horseshoe}
\alias{approx_horseshoe}
\title{Run approximate MCMC algorithm for horseshoe prior}
\usage{
approx_horseshoe(
  X,
  y,
  burn = 1000,
  iteration = 5000,
  auto.threshold = TRUE,
  threshold = 0,
  a = 0.2,
  b = 10,
  s = 0.8,
  tau = 1,
  sigma2 = 1,
  w = 1,
  alpha = 0.05
)
}
\arguments{
\item{X}{Design matrix, \eqn{X \in \mathbb{R}^{N \times p}}.}

\item{y}{Response vector, \eqn{y \in \mathbb{R}^{N}}.}

\item{iteration}{Number of samples to be drawn from the posterior.}

\item{threshold}{Threshold \eqn{\delta} to be used in the approximate MCMC
algorithm. If you select NULL(this is the default), the default is set
according to the sizes of N and p. if \eqn{p < N},
\eqn{\delta = 1/\sqrt{Np}}, else \eqn{\delta = 1/p}. Or, you can set the
value directly through this argument. For more information about
\eqn{\delta}, see \code{\link{Mhorseshoe}} and 4.1 of Johndrow et al.
(2020).}

\item{a}{Parameter of the rejection sampler, and it is recommended to leave
it at the default value, \eqn{a = 1/5}.}

\item{b}{Parameter of the rejection sampler, and it is recommended to leave
it at the default value, \eqn{b = 10}.}

\item{s}{\eqn{s^{2}} is the variance of the MH proposal distribution set by
the user. 0.8 is a good default. If set to 0, the algorithm proceeds by
fixing the global shrinkage parameter \eqn{\tau} to the initial setting
value.}

\item{tau}{Initial value of the global shrinkage parameter \eqn{\tau} when
starting the algorithm. If argument \eqn{s} is set to 0 and this argument
is set, analysis of a fixed \eqn{\tau} is performed. Default is 1.}

\item{sigma2}{Default for error variance \eqn{\sigma^{2}} is 1.}

\item{w}{Parameter of gamma prior for \eqn{\sigma^{2}}. Default is 1.}
}
\value{
\item{BetaSamples}{Samples from the posterior of the parameter
\eqn{\beta}.}
\item{LambdaSamples}{Lambda samples through rejection sampling.}
\item{TauSamples}{Tau samples through MH algorithm.}
\item{Sigma2Samples}{Samples from the posterior of the parameter
\eqn{sigma^{2}}.}
\item{ActiveSamples}{Samples recording the number of columns that satisfy
condition \eqn{\tau^{2}\lambda^{2}_{j} > \delta,\ j=1,2,...,p} in this
algorithm.}
}
\description{
In this function, The algorithm introduced in Section 2.2 of Johndrow et al.
(2020) is implemented, and is a horseshoe estimator that generally considers
the case where \eqn{p>>N}. The assumptions and notations for the model are
the same as those in \code{\link{Mhorseshoe}}. This algorithm introduces a
threshold and uses only a portion of the total \eqn{p} columns for matrix
multiplication, lowering the computational cost compared to the existing
horseshoe estimator. According to Section 3.2 of Johndrow et al. (2020), the
approximate MCMC algorithm applying the methodology constructs an
approximate Markov chain \eqn{P_{\epsilon}} that can converge to an exact
Markov chain \eqn{P}, and acceptable results were confirmed through
empirical analysis of simulation and real data.
}
\details{
This algorithm has the following changes compared to the exact algorithm:

\deqn{D_{\delta} = diag\left(\eta_{j}^{-1}1\left(\xi^{-1}\eta_{j}^{-1}
> \delta,\ j=1,2,...,p. \right) \right),}
\deqn{M_{\xi} \approx M_{\xi, \delta} = I_{N} + \xi^{-1}XD_{\delta}X^{T}.}

The set of columns that satisfies the condition(\eqn{\xi^{-1}\eta_{j}^{-1}
> \delta}) is defined as the active set, and let's define \eqn{S} as the
index set of the following columns.

\deqn{S = \{j\ |\ \xi^{-1}\eta_{j}^{-1} > \delta,\ j=1,2,...,p. \}.}

If \eqn{\xi^{-1}\eta_{j}^{-1}} is very small, the posterior of \eqn{\beta}
will have a mean and variance close to 0. Therefore, let's set
\eqn{\xi^{-1}\eta_{j}^{-1}} smaller than \eqn{\delta} to 0 and the size of
inverse \eqn{M_{\xi, \delta}} matrix is reduced as follows.

\deqn{length(S)=s_{\delta} \le p, \\ X_{S} \in R^{N \times s_{\delta}},
\quad D_{S} \in R^{s_{\delta} \times s_{\delta}}, \\ M_{\xi, \delta}^{-1} =
\left(I_{N} + \xi^{-1}X_{S}D_{S}X_{S}^{T} \right)^{-1}.}

\eqn{M_{\xi, \delta}^{-1}} can be expressed using the Woodbury identity
as follows.

\deqn{M_{\xi, \delta}^{-1} = I_{N} - X_{S}\left(\xi D_{S}^{-1} +
X_{S}^{T}X_{S} \right)^{-1}X_{S}^{T}.}

\eqn{M_{\xi, \delta}^{-1}}, which reduces the computational cost, is
applied to all parts of this algorithm, \eqn{\beta} samples are extracted
from the posterior using fast sampling(Bhattacharya et al.,2016) as follows.

\deqn{u \sim N_{p}(0, \xi^{-1}D),\quad f \sim N_{N}(0, I_{N}), \\
v = Xu + f,\quad v^{\star} = M_{\xi, \delta}^{-1}(y/\sigma - v), \\
\beta = \sigma(u + \xi^{-1}D_{\delta}X^{T}v^{\star}).}
}
\examples{
# Making simulation data.
set.seed(123)
N <- 50
p <- 100
true_beta <- c(rep(1, 10), rep(0, 90))

X <- matrix(1, nrow = N, ncol = p) # Design matrix X.
for (i in 1:p)
  X[, i] <- stats::rnorm(N, mean = 0, sd = 1)

y <- vector(mode = "numeric", length = N) # Response variable y.
e <- rnorm(N, mean = 0, sd = 2) # error term e.
for (i in 1:10)
  y <- y + true_beta[i] * X[, i]
y <- y + e

# Run with auto.threshold option
result <- approx_horseshoe(X, y, iteration = 500)

# Run with fixed threshold option
# result <- approx_horseshoe(X, y, iteration = 500, auto.threshold = FALSE, threshold = 1/5)

# posterior mean
post_mean <- apply(result$BetaSamples, MARGIN = 2, mean)

# Lower bound of the 95\% credible interval
post_leftCI <- apply(result$BetaSamples, MARGIN = 2,
                     quantile, probs = 0.025)

# Upper bound of the 95\% credible interval
post_rightCI <- apply(result$BetaSamples, MARGIN = 2,
                      quantile, probs = 0.975)

}
\references{
Bhattacharya, A., Chakraborty, A., & Mallick, B. K. (2016).
Fast sampling with Gaussian scale mixture priors in high-dimensional
regression. Biometrika, asw042.

Johndrow, J., Orenstein, P., & Bhattacharya, A. (2020).
Scalable Approximate MCMC Algorithms for the Horseshoe Prior. In Journal
of Machine Learning Research (Vol. 21).
}
